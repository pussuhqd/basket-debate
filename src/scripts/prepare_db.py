# src/scripts/prepare_db.py
"""
–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö products.db.

–≠—Ç–∞–ø—ã:
1. –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV ‚Üí SQLite (process_dataset)
2. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (cleanup)
3. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ mock —Ç–æ–≤–∞—Ä–æ–≤ (add_mocks)

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Embeddings –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ build_embeddings.py

–ó–∞–ø—É—Å–∫:
    # –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
    uv run python -m src.scripts.prepare_db
    
    # –¢–æ–ª—å–∫–æ —ç—Ç–∞–ø 1 (–æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV)
    uv run python -m src.scripts.prepare_db --step process
    
    # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å mock —Ç–æ–≤–∞—Ä—ã
    uv run python -m src.scripts.prepare_db --no-mocks
"""

import argparse
import json
import math
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import pandas as pd
from tqdm import tqdm

# ==================== –ò–ú–ü–û–†–¢–´ ====================
from src.utils.queries import get_connection, DB_PATH


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================

PROJECT_ROOT = Path(__file__).parent.parent.parent
INPUT_CSV = PROJECT_ROOT / "data" / "raw" / "russian_supermarket_prices.csv"
TAG_RULES_PATH = PROJECT_ROOT / "data" / "templates" / "tag_rules_extended.json"
MOCK_PRODUCTS_PATH = PROJECT_ROOT / "data" / "templates" / "mock.json"
MEAL_COMPONENTS_PATH = PROJECT_ROOT / "data" / "templates" /"meal_components_extended.json"

CHUNKSIZE = 50_000
MAX_REASONABLE_PRICE = 3000  # ‚ÇΩ/–∫–≥
USECOLS = ['product_name', 'product_category', 'brand', 'package_size', 'unit', 'new_price']

# –ò—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
EXCLUDED_CATEGORIES = [
    '–≥–µ–ª—å –¥–ª—è —Å—Ç–∏—Ä–∫–∏', '—Å—Ç–∏—Ä–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ—à–æ–∫', '–ø–æ—Ä–æ—à–æ–∫', '–≥–µ–ª—å',
    '–ø—è—Ç–Ω–æ–≤—ã–≤–æ–¥–∏—Ç–µ–ª—å', '—Å—Ä–µ–¥—Å—Ç–≤–æ –¥–ª—è –º—ã—Ç—å—è –ø–æ—Å—É–¥—ã', '–º–æ—é—â–µ–µ —Å—Ä–µ–¥—Å—Ç–≤–æ',
    '–±—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è', '–∫–æ—Ä–º –¥–ª—è –∫–æ—à–µ–∫', '–∫–æ—Ä–º –¥–ª—è —Å–æ–±–∞–∫', '–∫–æ—Ä–º –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö',
    '–∫–æ—Å–º–µ—Ç–∏–∫–∞', '—à–∞–º–ø—É–Ω—å', '–±–∞–ª—å–∑–∞–º', '–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä –¥–ª—è –≤–æ–ª–æ—Å',
    '–º—ã–ª–æ —Ç–≤–µ—Ä–¥–æ–µ', '–º—ã–ª–æ –∂–∏–¥–∫–æ–µ', '–º—ã–ª–æ', '–¥–µ–∑–æ–¥–æ—Ä–∞–Ω—Ç', '–∫—Ä–µ–º',
    '–∑—É–±–Ω–∞—è –ø–∞—Å—Ç–∞', '–∑—É–±–Ω–∞—è —â–µ—Ç–∫–∞', '–±—Ä–∏—Ç–≤–∞', '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –±—É–º–∞–≥–∞',
    '—Å–∞–ª—Ñ–µ—Ç–∫–∏', '–ø–æ–¥–≥—É–∑–Ω–∏–∫–∏', '–ø—Ä–æ–∫–ª–∞–¥–∫–∏'
]

# –ü–ª–æ—Ö–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
BAD_KEYWORDS = [
    '–ø–∞–ª—Ç—É—Å', '–∫–æ–Ω—Ñ–µ—Ç', '—à–æ–∫–æ–ª–∞–¥', '—á–∏–ø—Å', '—Å–Ω–µ–∫', '–∫–æ—Ä–º –¥–ª—è',
    '–º—ã–ª–æ', '—à–∞–º–ø—É–Ω—å', '–±—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è', '—Å—Ç–∏—Ä–∞–ª—å–Ω—ã–π', '–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å',
    '—Å–∞–ª—Ñ–µ—Ç–∫–∏', '–∏–≥—Ä—É—à–∫', '–¥–µ—Ç—Å–∫–æ–µ –ø–∏—Ç–∞–Ω–∏–µ', '–ø—é—Ä–µ "—Ñ—Ä—É—Ç–æ"', '–Ω–µ–∫—Ç–∞—Ä "—Ñ—Ä—É—Ç–æ"'
]


# ==================== –ó–ê–ì–†–£–ó–ö–ê –ü–†–ê–í–ò–õ ====================

def load_rules():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ —Ç–µ–≥–æ–≤ –∏ meal_components."""
    with open(TAG_RULES_PATH, 'r', encoding='utf-8') as f:
        tag_rules = json.load(f)
    
    with open(MEAL_COMPONENTS_PATH, 'r', encoding='utf-8') as f:
        meal_data = json.load(f)
    
    with open(MOCK_PRODUCTS_PATH, 'r', encoding='utf-8') as f:
        mock_data = json.load(f)

    return tag_rules, meal_data, mock_data


TAG_RULES, MEAL_DATA, MOCK_PRODUCTS = load_rules()


# ==================== –≠–¢–ê–ü 1: –û–ë–†–ê–ë–û–¢–ö–ê CSV ====================

def create_db_schema():
    """–°–æ–∑–¥–∞—ë—Ç –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É products."""
    conn = get_connection()
    conn.execute("DROP TABLE IF EXISTS products")
    conn.execute("""
        CREATE TABLE products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            product_name TEXT,
            product_category TEXT,
            brand TEXT,
            package_size REAL,
            unit TEXT,
            price_per_unit REAL,
            tags TEXT,
            meal_components TEXT,
            embedding BLOB
        )
    """)
    conn.commit()
    conn.close()
    print("   ‚úÖ –¢–∞–±–ª–∏—Ü–∞ products —Å–æ–∑–¥–∞–Ω–∞")


def clean_product_name(name: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —É–ø–∞–∫–æ–≤–∫–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è."""
    pattern = r'\s*\d+[.,]?\d*\s*(?:–≥|–º–ª|–ª|–∫–≥|—à—Ç|—É–ø|—É–ø–∞–∫–æ–≤–∫–∞|–ø–∞—á–∫–∞|–±—É—Ç|–±–∞–Ω–∫–∞)\b.*'
    cleaned = re.sub(pattern, '', str(name), flags=re.IGNORECASE).strip()
    return cleaned


def to_float(x) -> float:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ float."""
    try:
        return float(str(x).replace(',', '.'))
    except Exception:
        return math.nan


def normalize_price(price: float, size: float, unit: str) -> Tuple[float, float, Optional[str]]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ü–µ–Ω—É –ò —Ä–∞–∑–º–µ—Ä —É–ø–∞–∫–æ–≤–∫–∏ –∫ –±–∞–∑–æ–≤—ã–º –µ–¥–∏–Ω–∏—Ü–∞–º (–∫–≥, –ª, —à—Ç).
    
    Returns:
        (price_per_unit, normalized_size, normalized_unit)
    """
    if math.isnan(size) or size <= 0:
        return math.nan, math.nan, None
    
    unit = str(unit).lower().strip()
    price*=1.8
    if unit == '–≥':
        return round(price / size * 1000, 2), round(size / 1000, 3), '–∫–≥'
    elif unit == '–º–ª':
        return round(price / size * 1000, 2), round(size / 1000, 3), '–ª'
    elif unit == '–∫–≥':
        return round(price / size, 2), round(size, 3), '–∫–≥'
    elif unit == '–ª':
        return round(price / size, 2), round(size, 3), '–ª'
    elif unit == '—à—Ç':
        return round(price / size, 2), round(size, 3), '—à—Ç'
    else:
        return math.nan, math.nan, None



def extract_tags(product_name: str, product_category: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ tag_rules.json."""
    name = str(product_name).lower()
    category = str(product_category).lower()
    
    tags = set()
    
    for tag, rules in TAG_RULES.items():
        if not isinstance(rules, dict):
            continue
        
        for field, keywords in rules.items():
            if not isinstance(keywords, list):
                continue
            
            text = name if field == "name" else category
            
            if any(word in text for word in keywords):
                tags.add(tag)
                break
    
    return sorted(tags)


def assign_meal_components(product_name: str, product_category: str) -> List[str]:
    """–ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç meal_components (–º–∞–∫—Å–∏–º—É–º 2)."""
    name = str(product_name).lower()
    category = str(product_category).lower()
    text = f"{name} {category}"
    
    components = set()
    product_categories = MEAL_DATA.get('product_categories', {})
    
    for category_name, category_data in product_categories.items():
        keywords = category_data.get('name', [])
        
        for keyword in keywords:
            if keyword.lower() in text:
                meal_comps = category_data.get('attributes', {}).get('meal_components', [])
                components.update(meal_comps)
                break
    
    result = list(components)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    if len(result) > 2:
        priority_order = [
            'main_course', 'side_dish', 'beverage', 'salad',
            'bakery', 'sauce', 'dessert', 'snack'
        ]
        
        result_sorted = [comp for comp in priority_order if comp in result]
        result = result_sorted[:2]
    
    return result if result else ['other']


def is_valid_product(row, price_per_unit: float, normalized_unit: str) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞."""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
    if math.isnan(price_per_unit) or price_per_unit <= 0:
        return False, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ü–µ–Ω–∞"
    
    if price_per_unit > MAX_REASONABLE_PRICE:
        return False, f"–°–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
    if normalized_unit not in ['–∫–≥', '–ª', '—à—Ç']:
        return False, f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    category = str(row['product_category']).lower()
    for excluded in EXCLUDED_CATEGORIES:
        if excluded in category:
            return False, "–ò—Å–∫–ª—é—á—ë–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"
    
    return True, "OK"


def normalize_row(row) -> Optional[Dict]:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    name = clean_product_name(row['product_name'])
    size = to_float(row['package_size'])
    unit = row['unit']
    price = row['new_price']
    
    price_per_unit, normalized_size, normalized_unit = normalize_price(price, size, unit)
    is_valid, reason = is_valid_product(row, price_per_unit, normalized_unit)
    
    if not is_valid:
        return None
    
    tags = extract_tags(name, row['product_category'])
    meal_components = assign_meal_components(name, row['product_category'])
    
    return {
        "product_name": name,
        "product_category": row['product_category'],
        "brand": row['brand'],
        "package_size": normalized_size,  # ‚úÖ –¢–µ–ø–µ—Ä—å –≤ –∫–≥/–ª/—à—Ç
        "unit": normalized_unit,
        "price_per_unit": price_per_unit,
        "tags": "|".join(tags),
        "meal_components": "|".join(meal_components)
    }



def process_csv():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç CSV –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ –ë–î."""
    print("\n" + "=" * 70)
    print("üìä –≠–¢–ê–ü 1: –û–ë–†–ê–ë–û–¢–ö–ê CSV")
    print("=" * 70)
    
    if not INPUT_CSV.exists():
        print(f"‚ùå CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")
        return False
    
    create_db_schema()
    
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {INPUT_CSV}")
    print(f"–ú–∞–∫—Å. —Ü–µ–Ω–∞: {MAX_REASONABLE_PRICE}‚ÇΩ/–∫–≥")
    
    total_processed = 0
    total_loaded = 0
    conn = get_connection()
    
    for chunk_num, chunk in enumerate(pd.read_csv(INPUT_CSV, usecols=USECOLS, chunksize=CHUNKSIZE)):
        print(f"\nüì¶ –ß–∞–Ω–∫ {chunk_num + 1}: {len(chunk)} —Å—Ç—Ä–æ–∫")
        total_processed += len(chunk)
        
        chunk = chunk.dropna(subset=['product_name', 'new_price'])
        rows = []
        
        for _, row in chunk.iterrows():
            normalized = normalize_row(row)
            if normalized:
                rows.append(normalized)
        
        if rows:
            df = pd.DataFrame(rows)
            df.to_sql('products', conn, if_exists='append', index=False)
            total_loaded += len(rows)
            print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(rows)}")
    
    conn.close()
    
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total_loaded:,} —Ç–æ–≤–∞—Ä–æ–≤")
    print(f"‚ö†Ô∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_processed - total_loaded:,}")
    
    return True


# ==================== –≠–¢–ê–ü 2: –û–ß–ò–°–¢–ö–ê ====================

def cleanup_bad_products():
    """–£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã."""
    print("\n" + "=" * 70)
    print("üóëÔ∏è  –≠–¢–ê–ü 2: –û–ß–ò–°–¢–ö–ê –û–¢ –ú–£–°–û–†–ê")
    print("=" * 70)
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE id < 900000")
    before = cursor.fetchone()[0]
    
    print(f"–¢–æ–≤–∞—Ä–æ–≤ –¥–æ –æ—á–∏—Å—Ç–∫–∏: {before:,}")
    
    deleted_total = 0
    
    for keyword in BAD_KEYWORDS:
        cursor.execute(f"""
            DELETE FROM products
            WHERE id < 900000
            AND (product_name LIKE '%{keyword}%' OR product_category LIKE '%{keyword}%')
        """)
        deleted = cursor.rowcount
        if deleted > 0:
            print(f"   ‚ùå '{keyword}': {deleted}")
            deleted_total += deleted
    
    conn.commit()
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE id < 900000")
    after = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"\n‚úÖ –£–¥–∞–ª–µ–Ω–æ: {deleted_total} —Ç–æ–≤–∞—Ä–æ–≤")
    print(f"üìä –û—Å—Ç–∞–ª–æ—Å—å: {after:,} —Ç–æ–≤–∞—Ä–æ–≤")
    
    return True


# ==================== –≠–¢–ê–ü 3: MOCK –¢–û–í–ê–†–´ (–ë–ï–ó EMBEDDINGS) ====================


def add_mock_products():
    """–î–æ–±–∞–≤–ª—è–µ—Ç mock —Ç–æ–≤–∞—Ä—ã –ë–ï–ó embeddings."""
    print("\n" + "=" * 70)
    print("ü•ó –≠–¢–ê–ü 3: –î–û–ë–ê–í–õ–ï–ù–ò–ï MOCK –¢–û–í–ê–†–û–í")
    print("=" * 70)
    
    conn = get_connection()
    cursor = conn.cursor()
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ mock
    cursor.execute("DELETE FROM products WHERE id >= 900000")
    conn.commit()
    print("   üóëÔ∏è  –°—Ç–∞—Ä—ã–µ mock —É–¥–∞–ª–µ–Ω—ã")
    
    print(f"\nü•¶ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(MOCK_PRODUCTS)} —Ç–æ–≤–∞—Ä–æ–≤...")
    
    for product in tqdm(MOCK_PRODUCTS, desc="Mock —Ç–æ–≤–∞—Ä—ã"):
        cursor.execute("""
            INSERT INTO products
            (id, product_name, product_category, brand, price_per_unit, unit,
             package_size, tags, meal_components)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            product['id'], product['name'], product['category'], product['brand'],
            product['price'], product['unit'], product['size'],
            product['tags'], "|".join(product['components'])
        ))
    
    conn.commit()
    conn.close()
    
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {len(MOCK_PRODUCTS)} —Ç–æ–≤–∞—Ä–æ–≤ (–±–µ–∑ embeddings)")
    print(f"‚ÑπÔ∏è  –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings –∑–∞–ø—É—Å—Ç–∏—Ç–µ: uv run python -m src.scripts.build_embeddings")
    
    return True


# ==================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    parser = argparse.ArgumentParser(description='–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ë–î products.db')
    parser.add_argument(
        '--step',
        choices=['process', 'cleanup', 'mocks', 'all'],
        default='all',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç—Ç–∞–ø'
    )
    parser.add_argument(
        '--no-mocks',
        action='store_true',
        help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ mock —Ç–æ–≤–∞—Ä–æ–≤'
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("üöÄ –ü–ê–ô–ü–õ–ê–ô–ù –ü–û–î–ì–û–¢–û–í–ö–ò –ë–ê–ó–´ –î–ê–ù–ù–´–•")
    print("=" * 70)
    print(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DB_PATH}")
    print("=" * 70)
    
    success = True
    
    # –≠—Ç–∞–ø 1
    if args.step in ['process', 'all']:
        success = process_csv()
        if not success:
            return
    
    # –≠—Ç–∞–ø 2
    if args.step in ['cleanup', 'all']:
        success = cleanup_bad_products()
        if not success:
            return
    
    # –≠—Ç–∞–ø 3
    if args.step in ['mocks', 'all'] and not args.no_mocks:
        success = add_mock_products()
        if not success:
            return
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 70)
    print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 70)
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE id < 900000")
    real_count = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE id >= 900000")
    mock_count = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM products WHERE embedding IS NOT NULL")
    with_embeddings = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"–†–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {real_count:,}")
    print(f"Mock —Ç–æ–≤–∞—Ä–æ–≤: {mock_count}")
    print(f"–° embeddings: {with_embeddings:,}")
    print(f"–ë–µ–∑ embeddings: {(real_count + mock_count) - with_embeddings:,}")
    print("=" * 70)
    print("‚úÖ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–Å–ù")
    
    if with_embeddings == 0:
        print("\n‚ö†Ô∏è  –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
        print("   uv run python -m src.scripts.build_embeddings")
    
    print("=" * 70)


if __name__ == "__main__":
    main()
