# src/nlp/llm_parser.py
import json
import re
from typing import Dict, Any, Optional
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"
)


def build_manual_prompt(user_query: str) -> str:
    """
    –í—Ä—É—á–Ω—É—é —Å–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏.
    """
    function_schema = {
        "name": "parse_basket_query",
        "description": "–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∫–æ—Ä–∑–∏–Ω–µ. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –í–´–ó–û–í–ò –≠–¢–£ –§–£–ù–ö–¶–ò–Æ –¥–ª—è –ª—é–±–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –æ –µ–¥–µ, –ø–æ–∫—É–ø–∫–∞—Ö –∏–ª–∏ –∫–æ—Ä–∑–∏–Ω–µ.",
        "parameters": {
            "type": "object",
            "properties": {
                "budget_rub": {
                    "type": "integer",
                    "description": "–ë—é–¥–∂–µ—Ç –≤ —Ä—É–±–ª—è—Ö. –ü—Ä–∏–º–µ—Ä—ã: '–∑–∞ 1500', '–¥–æ 2000', '–º–∞–∫—Å–∏–º—É–º 3000'. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –Ω–µ –≤–∫–ª—é—á–∞–π —ç—Ç–æ –ø–æ–ª–µ."
                },
                "people": {
                    "type": "integer",
                    "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫. –ü—Ä–∏–º–µ—Ä—ã: '–Ω–∞ –¥–≤–æ–∏—Ö' (2), '–¥–ª—è —Ç—Ä–æ–∏—Ö' (3), '–¥–ª—è —Å–µ–º—å–∏ –∏–∑ 4' (4). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –Ω–µ –≤–∫–ª—é—á–∞–π —ç—Ç–æ –ø–æ–ª–µ."
                },
                "horizon_value": {
                    "type": "integer",
                    "description": "–ß–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ä–æ–∫–∞. –ü—Ä–∏–º–µ—Ä—ã: '–Ω–∞ –¥–µ–Ω—å' (1), '–Ω–∞ –Ω–µ–¥–µ–ª—é' (7), '–Ω–∞ –º–µ—Å—è—Ü' (30). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –Ω–µ –≤–∫–ª—é—á–∞–π —ç—Ç–æ –ø–æ–ª–µ."
                },
                "horizon_unit": {
                    "type": "string",
                    "enum": ["day", "week", "month"],
                    "description": "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å—Ä–æ–∫–∞. '–¥–µ–Ω—å' -> day, '–Ω–µ–¥–µ–ª—è' -> week, '–º–µ—Å—è—Ü' -> month. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –Ω–µ –≤–∫–ª—é—á–∞–π —ç—Ç–æ –ø–æ–ª–µ."
                },
                "meal_types": {
                    "type": "array",
                    "items": {"type": "string", "enum": ["breakfast", "lunch", "dinner", "snack"]},
                    "description": "–¢–∏–ø –ø—Ä–∏—ë–º–∞ –ø–∏—â–∏. '–∑–∞–≤—Ç—Ä–∞–∫' -> breakfast, '–æ–±–µ–¥' -> lunch, '—É–∂–∏–Ω' -> dinner, '–ø–µ—Ä–µ–∫—É—Å' -> snack. –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []."
                },
                "exclude_tags": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "–ü—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–£–ñ–ù–û –ò–°–ö–õ–Æ–ß–ò–¢–¨. '–±–µ–∑ –º–æ–ª–æ–∫–∞' -> ['dairy'], '–±–µ–∑ –º—è—Å–∞' -> ['meat'], '–±–µ–∑ —Å–∞—Ö–∞—Ä–∞' -> ['no_sugar'], '–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞' -> ['gluten_free']. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []."
                },
                "include_tags": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "–ü—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å. '–≤–µ–≥–∞–Ω' -> ['vegan'], '—Ö–∞–ª—è–ª—å' -> ['halal'], '–¥–µ—Ç—Å–∫–æ–µ' -> ['children_goods']. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []."
                },
                "max_time_min": {
                    "type": "integer",
                    "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö. '–∑–∞ 30 –º–∏–Ω—É—Ç' -> 30, '–º–∞–∫—Å–∏–º—É–º —á–∞—Å' -> 60, '–±—ã—Å—Ç—Ä—ã–π —É–∂–∏–Ω' -> 30."
                },
                "prefer_quick": {
                    "type": "boolean",
                    "description": "True, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –±—ã—Å—Ç—Ä–æ–µ –±–ª—é–¥–æ: '–±—ã—Å—Ç—Ä–æ', '–Ω–∞ —Å–∫–æ—Ä—É—é —Ä—É–∫—É', '–±—ã—Å—Ç—Ä—ã–π —É–∂–∏–Ω'."
                },
                "prefer_cheap": {
                    "type": "boolean",
                    "description": "True, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –¥–µ—à–µ–≤–æ/–±—é–¥–∂–µ—Ç–Ω–æ: '–¥–µ—à–µ–≤–æ', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–±—é–¥–∂–µ—Ç–Ω–æ', '–ø–æ–¥–µ—à–µ–≤–ª–µ'."
                }
            }
        }
    }
    
    system_message = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –ø–æ–∫—É–ø–∫–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤. 

–í–°–ï–ì–î–ê –≤—ã–∑—ã–≤–∞–π —Ñ—É–Ω–∫—Ü–∏—é parse_basket_query –¥–ª—è –õ–Æ–ë–û–ì–û –∑–∞–ø—Ä–æ—Å–∞ –æ –µ–¥–µ –∏–ª–∏ –∫–æ—Ä–∑–∏–Ω–µ.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —ç—Ç–æ–º—É):
–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: {{"name": "parse_basket_query", "arguments": {{...}}}}

–î–æ—Å—Ç—É–ø–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: {json.dumps(function_schema, ensure_ascii=False)}

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}"""
    
    return system_message


def extract_function_call(generated_text: str) -> Optional[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.
    """
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 1: "–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: {...}"
    pattern1 = r'–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏:\s*(\{.*?\})(?:\s*<end_of_turn>|$)'
    match = re.search(pattern1, generated_text, re.DOTALL)
    
    if match:
        try:
            json_str = match.group(1)
            function_call = json.loads(json_str)
            if function_call.get("name") == "parse_basket_query":
                return function_call
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSONDecodeError (pattern1): {e}")
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 2: "–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ {...}" (–±–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏—è)
    pattern2 = r'–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏\s+(\{.*?\})(?:\s*<end_of_turn>|$)'
    match = re.search(pattern2, generated_text, re.DOTALL)
    
    if match:
        try:
            json_str = match.group(1)
            function_call = json.loads(json_str)
            if function_call.get("name") == "parse_basket_query":
                return function_call
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSONDecodeError (pattern2): {e}")
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 3: "–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ parse_basket_query —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {...}"
    pattern3 = r'–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏\s+parse_basket_query\s+—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\s*(\{.*?\})(?:\s*<end_of_turn>|$)'
    match = re.search(pattern3, generated_text, re.DOTALL)
    
    if match:
        try:
            arguments = json.loads(match.group(1))
            function_call = {
                "name": "parse_basket_query",
                "arguments": arguments
            }
            return function_call
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSONDecodeError (pattern3): {e}")
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 4: –ü–æ–∏—Å–∫ —Å –ø–æ–¥—Å—á—ë—Ç–æ–º —Å–∫–æ–±–æ–∫
    match = re.search(r'\{[^{]*?"name"\s*:\s*"parse_basket_query"', generated_text, re.DOTALL)
    
    if match:
        try:
            start_pos = match.start()
            brace_count = 0
            end_pos = start_pos
            
            for i, char in enumerate(generated_text[start_pos:], start=start_pos):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_pos = i + 1
                        break
            
            json_str = generated_text[start_pos:end_pos]
            function_call = json.loads(json_str)
            return function_call
        except (json.JSONDecodeError, ValueError) as e:
            print(f"[ERROR] JSONDecodeError (pattern4): {e}")
    
    print(f"[ERROR] –ù–µ –Ω–∞–π–¥–µ–Ω JSON –≤ –æ—Ç–≤–µ—Ç–µ:")
    print(generated_text[:300])
    return None


def parse_query_with_function_calling(user_query: str) -> Dict[str, Any]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    """
    prompt = build_manual_prompt(user_query)
    
    try:
        response = client.chat.completions.create(
            model="gemma-2-9b-it-russian-function-calling",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.05,
            max_tokens=512,
            stop=["<end_of_turn>"]
        )
        
        generated_text = response.choices[0].message.content
        print(f"[DEBUG] LLM Response: {generated_text}")
        
        function_call = extract_function_call(generated_text)
        
        if not function_call or function_call.get("name") != "parse_basket_query":
            print("[WARNING] –ú–æ–¥–µ–ª—å –Ω–µ –≤—ã–∑–≤–∞–ª–∞ —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
            print(f"[DEBUG] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π function_call: {function_call}")
            return _empty_result(user_query)
        
        args = function_call.get("arguments", {})
        
        result = {
            "raw_text": user_query,
            "budget_rub": args.get("budget_rub"),
            "people": args.get("people"),
            "horizon": None,
            "meal_type": args.get("meal_types", []),
            "exclude_tags": args.get("exclude_tags", []),
            "include_tags": args.get("include_tags", []),
            "max_time_min": args.get("max_time_min"),
            "prefer_quick": args.get("prefer_quick", False),
            "prefer_cheap": args.get("prefer_cheap", False),
        }
        
        if args.get("horizon_value") and args.get("horizon_unit"):
            result["horizon"] = {
                "value": args["horizon_value"],
                "unit": args["horizon_unit"]
            }
        
        return result
        
    except Exception as e:
        print(f"[ERROR] LLM Error: {e}")
        import traceback
        traceback.print_exc()
        return _empty_result(user_query)


def _empty_result(user_query: str) -> Dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
    return {
        "raw_text": user_query,
        "budget_rub": None,
        "people": None,
        "horizon": None,
        "meal_type": [],
        "exclude_tags": [],
        "include_tags": [],
        "max_time_min": None,
        "prefer_quick": False,
        "prefer_cheap": False,

    }

def test_parser():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–µ—Ä."""
    print("=" * 70)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï LLM Parser")
    print("=" * 70)
    
    test_queries = [
        "–£–∂–∏–Ω –Ω–∞ –¥–≤–æ–∏—Ö –∑–∞ 1500 —Ä—É–±–ª–µ–π",
        "–í–µ–≥–∞–Ω—Å–∫–∏–π –∑–∞–≤—Ç—Ä–∞–∫ –Ω–∞ —Ç—Ä–æ–∏—Ö",
        "–û–±–µ–¥ –±–µ–∑ –º–æ–ª–æ—á–∫–∏ –∏ —Ä—ã–±—ã",
        "–ë—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–µ–∫—É—Å –Ω–∞ 500 —Ä—É–±–ª–µ–π"
    ]
    
    for query in test_queries:
        print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
        result = parse_query_with_function_calling(query)
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {json.dumps(result, ensure_ascii=False, indent=2)}")


if __name__ == "__main__":
    test_parser()